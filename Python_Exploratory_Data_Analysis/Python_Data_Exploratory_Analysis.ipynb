{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learningweek.upgrad.com/v/course/140/session/11518/segment/56887\n",
    "\n",
    "### Python  Exploratory Data Analysis, or EDA.  \n",
    "\n",
    "In this module, we will cover the following topics:\n",
    "\n",
    "- Data sourcing\n",
    "- Data cleaning\n",
    "- Univariate analysis\n",
    "- Bivariate analysis\n",
    "- Derived metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Introduction\n",
    "Welcome to the session on \"Data sourcing\".\n",
    "\n",
    " \n",
    "\n",
    "In this session:\n",
    "To solve a business problem using analytics, you need to have historical data. Data is the key — the better the data, the more insights you can get out of it.\n",
    "\n",
    "\n",
    "Typically, data comes from various sources and your first job as a data analyst is to procure the data from them. In this session, you will learn about various sources of data and how to source data from public and private sources. The broad agenda for this session is as follows:\n",
    "\n",
    "- Private Data\n",
    "- Public Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Data\n",
    "A large number of organisations seek to leverage data analytics to make crucial decisions. As organisations become customer-centric, they utilise insights from data to enhance customer experience, while also optimising their daily processes.\n",
    "\n",
    " \n",
    "\n",
    "Let's listen to Anand about how data in the banking, telecom and HR industries looks like and how it helps the companies make crucial decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You learnt the use of data in the banking, telecom and human resources sectors. While banks use data to make credit related decisions, telecoms use data to optimise plans for customers and predict customer churn. HR data analytics helps identify and predict employee behaviour.\n",
    "\n",
    " \n",
    "\n",
    "Let us now look at the applications of data analytics in retail and media, and what this data looks li`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Banking\n",
    "- Telecom\n",
    "- HR Data\n",
    "    - employees who are likely to leave\n",
    "    - who can be a good leader\n",
    "    \n",
    "- Retail\n",
    "   - product pricing\n",
    "   - product purchasing\n",
    "   - product stocking\n",
    "   - product that sell well together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retail - Market Basket Analysis\n",
    "You saw an example of 'products that sell well together' (in this case the prepaid top-up plans that add up to a multiple of 100). The study of such groups of products falls under 'market basket analysis'.\n",
    "\n",
    "Think of at least two examples from your experience where you have seen some products that sell well together. Write your answer in the text box below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Round digit price\n",
    "2. Less prices \n",
    "3. Complimentary products - for eg Facewash + Cream\n",
    "\n",
    "\n",
    "- TV\n",
    "  1. trp/ad\n",
    "- Social Media\n",
    "   - sentiment analysis from tweets\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Data\n",
    "Previously, we talked about various private data sources. Now, let’s learn how to source public data sets. Public data is available on the internet on various platforms. A lot of data sets are available for direct analysis, whereas some of the data have to be manually extracted and converted into a format that is fit for analysis.\n",
    "\n",
    "- Public data sets - awesome-public-datasets on github \n",
    "- government data\n",
    "  1. data.gov\n",
    "  2. data.gov.in\n",
    "  3. data.gov.uk\n",
    "- census data \n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the session on ‘Data Cleaning’.\n",
    "\n",
    " \n",
    "\n",
    "In the previous session, you learnt about various sources of data. Once you have procured the data, the next step is to clean it to get rid of data quality issues.\n",
    "\n",
    " \n",
    "\n",
    "There are various types of quality issues when it comes to data, and that’s why data cleaning is one of the most time-consuming steps of data analysis. For example, there could be formatting errors (e.g. rows and columns are ill-formatted, unclearly named etc.), missing values, repeated rows, spelling inconsistencies etc. These issues could make it difficult to analyse data and could lead to errors or irrelevant results. Thus, these issues need to be corrected before data is analysed.\n",
    "\n",
    "​​​​​\n",
    "\n",
    "In this session\n",
    "You will learn how to identify the various quality issues and techniques to clean data.\n",
    "\n",
    " \n",
    "\n",
    "Though data cleaning is often done in a somewhat haphazard way and it is too difficult to define a ‘single structured process’, we will study data cleaning in the following steps:\n",
    "\n",
    "### Checklist for Fixing Rows\n",
    "\n",
    "- Delete summary rows: Total, Subtotal rows\n",
    "- Delete incorrect rows: Header rows, Footer rows\n",
    "- Delete extra rows: Column number, indicators, Blank rows, Page No.\n",
    "\n",
    "### Checklist for Fixing Columns\n",
    "\n",
    "- Merge columns for creating unique identifiers if needed: E.g. Merge State, City into Full address\n",
    "- Split columns for more data: Split address to get State and City to analyse each separately\n",
    "- Add column names: Add column names if missing\n",
    "\n",
    "Rename columns consistently: Abbreviations, encoded columns\n",
    "\n",
    "Delete columns: Delete unnecessary columns\n",
    "\n",
    "Align misaligned columns: Dataset may have shifted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLAs\n",
    "In a database “MLA” containing the details of MLAs throughout India, you have a city named Rampur in Himachal Pradesh, UP and Chhattisgarh. The city names are stored in the “City” column while state names are stored in the “State” column. What is the best way to represent the cities in this case? \n",
    "\n",
    "choose\n",
    "- 1. Keeping the most relevant Rampur, removing the other two\n",
    "\n",
    "- 2. Merging the City, State columns to get a unique identifier\n",
    "\n",
    "- 3. Randomly choosing which Rampur to keep in the data set, to make the data set simpler\n",
    "\n",
    "answer -2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### missing values\n",
    "\n",
    "The most important takeaway from this lecture is - good methods add information, bad methods exxagerate information.\n",
    "\n",
    " \n",
    "\n",
    "In case you can add information from reliable external sources, you should use it to replace missing values. But often, it is better to let missing values be and continue with the analysis rather than extrapolate the available information.\n",
    "\n",
    "\n",
    "Let us summarise how to deal with missing values:\n",
    "\n",
    "Set values as missing values: Identify values that indicate missing data, and yet are not recognised by the software as such, e.g treat blank strings, \"NA\", \"XX\", \"999\", etc. as missing.\n",
    "\n",
    "Adding is good, exaggerating is bad: You should try to get information from reliable external sources as much as possible, but if you can’t, then it is better to keep missing values as such rather than exaggerating the existing rows/columns.\n",
    "\n",
    "Delete rows, columns: Rows could be deleted if the number of missing values are insignificant in number, as this would not impact the analysis. Columns could be removed if the missing values are quite significant in number.\n",
    "\n",
    "Fill partial missing values using business judgement: Missing time zone, century, etc. These values are easily identifiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardising Values\n",
    "\n",
    "\n",
    "Scaling ensures that the values have a common scale, which makes analysis easier. E.g. let's take a data set containing the grades of students studying at different universities. Some of the universities give grades on a scale of 4, while others give grades on a scale of 10. Therefore, you cannot assume that a GPA of 3 on a scale of 4 is equal to a GPA of 3 on a scale of 10, even though they are same quantitatively. Thus, for the purpose of analysis, these values need to be brought to a common scale, such as the percentage scale.\n",
    "\n",
    "\n",
    "One of the concepts that surely caught your attention is outliers. Removing outliers is an important step in data cleaning. An outlier may disproportionately affect the results of your analysis. This may lead to faulty interpretations. It is also important to understand that there is no fixed definition of an outlier. It is left up to the judgment of the analyst to decide the criteria on which data would be categorised as abnormal or an outlier. We will look into one such method in the next session.\n",
    "\n",
    "\n",
    "Let’s summarise what you learnt about standardising variables. You could use this as a checklist for future data cleaning exercises.\n",
    "\n",
    "Standardise units: Ensure all observations under a variable have a common and consistent unit, e.g. convert lbs to kgs, miles/hr to km/hr, etc.\n",
    "\n",
    "Scale values if required:  Make sure the observations under a variable have a common scale\n",
    "\n",
    "Standardise precision for better presentation of data, e.g. 4.5312341 kgs to 4.53 kgs.\n",
    "\n",
    "Remove outliers: Remove high and low values that would disproportionately affect the results of your analysis.\n",
    "\n",
    " \n",
    "\n",
    "Now that you have learned how to standardise numeric values, let us see how standardising text values is equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarize\n",
    "\n",
    "Let us summarise what you learnt about standardising text. You could use this as a checklist for future data cleaning exercises.\n",
    "\n",
    "- Remove extra characters like such as common prefix/suffix, leading/trailing/multiple spaces, etc. These are irrelevant to analysis.\n",
    "\n",
    "- Standardise case: There are various cases that string variables may take, e.g. UPPERCASE, lowercase, Title Case, Sentence case, etc.\n",
    "\n",
    "- Standardise format: E.g. 23/10/16 to 2016/10/20, “Modi, Narendra\" to “Narendra Modi\", etc.\n",
    "\n",
    "In the next session, you will learn how to fix invalid values.\n",
    "\n",
    "http://r-statistics.co/Outlier-Treatment-With-R.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invalid values\n",
    "\n",
    "\n",
    "Invalid Values\n",
    "In the previous segment, you learnt how to standardise values. When standardising values, you do not really pay attention to the validity of the actual values of the variables. This is what we will discuss now as you learn to fix invalid values.\n",
    "\n",
    " \n",
    "\n",
    "A data set can contain invalid values in various forms. Some of the values could be truly invalid, e.g. a string “tr8ml” in a variable containing mobile numbers would make no sense and hence would be better removed. Similarly, a height of 11 ft would be an invalid value in a set containing heights of children.\n",
    "\n",
    " \n",
    "\n",
    "On the other hand, some invalid values can be corrected. E.g. a numeric value with a data type of string could be converted to its original numeric type. Issues might arise due to python misinterpreting the encoding of a file, thus showing junk characters where there were valid characters. This could be corrected by correctly specifying the encoding or converting the data set to the accurate format before importing.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an invalid value problem, and you do not know what accurate values could replace the invalid values, it is recommended to treat these values as missing. E.g. in the case of a string “tr8ml” in a Contact column, it is recommended to remove the invalid value and treat it as a missing value.\n",
    "\n",
    " \n",
    "\n",
    "Let’s summarise what you learnt about fixing invalid values. You could use this as a checklist for future data cleaning exercises.\n",
    "\n",
    "- Encode unicode properly: In case the data is being read as junk characters, try to change encoding, E.g. CP1252 instead of UTF-8.\n",
    "- Convert incorrect data types: Correct the incorrect data types to the correct data types for ease of analysis. E.g. if numeric values are stored as strings, it would not be possible to calculate metrics such as mean, median, etc. Some of the common data type corrections are — string to number: \"12,300\" to “12300”; string to date: \"2013-Aug\" to “2013/08”; number to string: “PIN Code 110001” to \"110001\"; etc.\n",
    "- Correct values that go beyond range: If some of the values are beyond logical range, e.g. temperature less than   -273° C (0° K), you would need to correct them as required. A close look would help you check if there is scope for correction, or if the value needs to be removed.\n",
    "- Correct values not in the list: Remove values that don’t belong to a list. E.g. In a data set containing blood groups of individuals, strings “E” or “F” are invalid values and can be removed.\n",
    "- Correct wrong structure: Values that don’t follow a defined structure can be removed. E.g. In a data set containing pin codes of Indian cities, a pin code of 12 digits would be an invalid value and needs to be removed. Similarly, a phone number of 12 digits would be an invalid value.\n",
    "- Validate internal rules: If there are internal rules such as a date of a product’s delivery must definitely be after the date of the order, they should be correct and consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "\n",
    "\n",
    "In the previous segment, you learnt how to go about fixing invalid values. Now, let’s learn how to filter data.\n",
    "\n",
    " \n",
    "\n",
    "After you have fixed the missing values, standardised the existing values, and fixed the invalid values, you would get to the last stage of data cleaning. Though you have a largely accurate data set by now, you might not need the entire data set for your analysis. It is important to understand what you need to infer from the data and then choose the relevant parts of the data set for your analysis. Thus, you need to filter the data to get what you need for your analysis.\n",
    "\n",
    "Let’s summarise what you learnt about filtering data. You could use this as a checklist for future data cleaning exercises.\n",
    "\n",
    "- Deduplicate data: Remove identical rows, remove rows where some columns are identical\n",
    "- Filter rows: Filter by segment, filter by date period to get only the rows relevant to the analysis\n",
    "- Filter columns: Pick columns relevant to the analysis\n",
    "- Aggregate data: Group by required keys, aggregate the rest\n",
    " \n",
    "\n",
    "Comprehension\n",
    "\n",
    "You have two data sets, each containing different columns of information regarding the employees of a company. The two data sets have a varying number of rows/observations. Dataset-1 contains information on Name, Age, and Salary against Employee ID. Dataset-2 contains information on Education and Experience against EmployeeID. It is unknown why the data sets have varying rows, i.e. why some employees were missed while collecting the data for each of the data sets. Employee ID is the identifier and thus the common column in both the data sets.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q. You are required to merge the two datasets such that only the employees common in both the data sets are a part of the new combined data set.\n",
    "\n",
    "What operation would you use to get the desired result?\n",
    "\n",
    "\n",
    "a. Inner Join\n",
    "Feedback : Inner join is used to merge the data sets, only keeping the rows corresponding to employee IDs that are common to both the data sets while removing the rest. Outer join, on the other hand, is used when you want to display all the rows from both the tables (even if the rows do not have a common key in both the tables)\n",
    "Correct\n",
    "\n",
    "\n",
    "\n",
    "b. Cross join selects all the rows from both the tables and returns a set where each row of first table is joined with each row of second table, The question requires a merge that returns all unique rows from both the tables.\n",
    "\n",
    "\n",
    "c. Full outer join\n",
    "Feedback : Full outer join is used to merge the data sets such that, apart from keeping the common Employee IDs, it also includes the Employee IDs unique to both the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify all data quality issues in internet dataset\n",
    "\n",
    "\n",
    "- It contains unnecessary header row(s)\n",
    "\n",
    "- Lowercase and uppercase issues\n",
    "\n",
    "- It contains blank values, not interpreted as NA\n",
    "\n",
    "- It contains duplicated rows\n",
    "\n",
    "- It contains a missing column name\n",
    "\n",
    "- It contains a summary row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis\n",
    "\n",
    "\n",
    "\n",
    "Welcome to the session on ‘Univariate Analysis’. In the previous sessions, you learnt how to source data and how to clean it for analysis.\n",
    "\n",
    " \n",
    "\n",
    "In this session\n",
    "As the term “univariate” suggests, this session deals with analysing variables one at a time. It is important to separately understand each variable before moving on to analysing multiple variables together.\n",
    "\n",
    " \n",
    "\n",
    "The broad agenda for this session is as follows:\n",
    "\n",
    "- Metadata description\n",
    "\n",
    "- Data distribution plots\n",
    "\n",
    "- Summary metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Variables\n",
    "\n",
    " \n",
    "\n",
    "You learnt the difference between ordered and unordered categorical variables -\n",
    "\n",
    "Ordered ones have some kind of ordering. Some examples are\n",
    "Salary = High-Medium-low\n",
    "Month = Jan-Feb-Mar etc.\n",
    "Unordered ones do not have the notion of high-low, more-less etc. Example:\n",
    "Type of loan taken by a person = home, personal, auto etc.\n",
    "Organisation of a person = Sales, marketing, HR etc.\n",
    "Apart from the two types of categorical variables, the other most common type is quantitative variables. These are simply numeric variables which can be added up, multiplied, divided etc. For example, salary, number of bank accounts, runs scored by a batsman, the mileage of a car etc.\n",
    "\n",
    " \n",
    "\n",
    "So far, we have discussed the following types of variables:\n",
    "\n",
    "\n",
    "\n",
    "- Categorical variables\n",
    "Unordered \n",
    "Ordered\n",
    "- Quantitative / numeric variables\n",
    " \n",
    "\n",
    "\n",
    "q. Ordered and Unordered Categorical Variables\n",
    "Categorical variables can be of two types - ordered categorical and unordered categorical. In unordered, it is not possible to say that a certain category is 'more or less' or 'higher or lower' than others. For example, color is such a variable (red is not greater or more than green etc.)\n",
    "\n",
    "On the other hand, ordered categories have a notion of 'higher-lower', 'before-after', 'more-less' etc. For e.g. the age-group variable having three values - child, adult and old is ordered categorical because an old person is 'more aged' than an adult etc. In general, it is possible to define some kind of ordering.\n",
    "\n",
    "Which of the following variables are ordered categorical?\n",
    "\n",
    "\n",
    "a. Types of loan taken by a person - home loan, education loan, personal loan\n",
    "\n",
    "b. The country of a person\n",
    "\n",
    "c. The months in a year - Jan, Feb, March etc.  - corect\n",
    "\n",
    "d. The organisation of a person - HR, marketing, analytics etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q2. Categorical Variables\n",
    "A survey variable called “StateofMind” contains values 1 to 4 denoting the different states of mind of the respondents.\n",
    "\n",
    " StateofMind\tMeaning\n",
    "- 1\t             Confused\n",
    "- 2\t             Happy\n",
    "- 3\t             Depressed\n",
    "- 4\t             Confident\n",
    " \n",
    "\n",
    "Which of the following categories would the variable “StateofMind” fall into?\n",
    "\n",
    "\n",
    "- a. Ordered categorical\n",
    "\n",
    "- b. Unordered categorical\n",
    "Feedback : Don't get confused by the numeric values for StateofMind. There is no order between the categories.\n",
    "Correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank Frequency, log log plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unordered Categorical Variables - Univariate Analysis\n",
    "Let’s now move to the most interesting part of EDA — getting useful insights from the data. So far, you have seen two types of variables - categorical (ordered / unordered) and quantitative (or numeric). In this segment, you will learn how to conduct univariate analysis on unordered categorical variables. \n",
    "\n",
    " You saw how one can use plots to extract meaningful information from unordered categorical variables. Compare the answer you had given to the question before the lecture - how would your approach of analysing unordered categorical variables change after studying this?\n",
    "\n",
    " \n",
    "\n",
    "It is important to note that rank-frequency plots enable you to extract meaning even from seemingly trivial unordered categorical variables such as country, name of an artist, name of a github user etc.\n",
    "\n",
    " \n",
    "\n",
    "The objective here is not to put excessive focus on power laws or rank-frequency plots, but rather to understand that non-trivial analysis is possible even on unordered categorical variables, and that plots can help you out in that process.\n",
    "\n",
    " Let us now see how a power law distribution is created in Excel.\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " Why plotting on a log-log scale helps\n",
    "\n",
    " \n",
    "\n",
    "The objective of using a log scale is to make the plot readable by changing the scale. For example, the first ranked item had a frequency of 29000, the second ranked had 3500, the seventh had 700 and most others had very low frequencies such as 100, 80, 21 etc.  The range of frequencies is too large to fit on the plot.\n",
    "\n",
    " \n",
    "\n",
    "Plotting on a log scale compresses the values to a smaller scale which makes the plot easy to read.\n",
    "\n",
    " \n",
    "\n",
    "This happens because log(x) is a much smaller number than x. For example, log(10) = 1, log(100) = 2, log(1000) = 3 and so on. Thus, log(29000) is now approx. 4.5, log(3500) is approx. 3.5 and so on. What was earlier varying from 29000 to 1 is now compressed between 4.5 and 0, making the values easier to read on a plot.\n",
    "\n",
    " \n",
    "\n",
    "To summarise, the major takeaways from this lecture are:\n",
    "\n",
    "Plots are immensely helpful in identifying hidden patterns in the data \n",
    "It is possible to extract meaningful insights from unordered categorical variables using rank-frequency plots\n",
    "Rank-frequency plots of unordered categorical variables, when plotted on a log-log scale, typically result in a power law distribution\n",
    " \n",
    "\n",
    "In the next lecture, you will study how to conduct univariate analysis on ordered categorical variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
