{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to this module on Python for Data Science.\n",
    "\n",
    "  \n",
    "\n",
    "In this module\n",
    "In this module, we will be covering NumPy and Pandas that are the most used Python libraries for Data Science.\n",
    "\n",
    " \n",
    "\n",
    "This module will teach you the basics of NumPy which is the fundamental package for scientific computing in Python. NumPy consists of a powerful data structure called multidimensional arrays. Pandas is another powerful Python library that provides fast and easy data analysis platform. You will also learn about 'Data Visualisation in Python' in a separate module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "### Pre-requisite for this Module\n",
    "The students are expected to have gone through \"Intro to Python\" module before beginning with this module.\n",
    "\n",
    " \n",
    "\n",
    "In this session\n",
    "Understand advantages of vectorized code using Numpy (over standard python ways)\n",
    "Create NumPy arrays\n",
    "Convert lists and tuples to NumPy arrays\n",
    "Create (initialise) arrays\n",
    "Inspect the structure and content of arrays\n",
    "Subset, slice, index and iterate through arrays\n",
    "Compare computation times in NumPy and standard Python lists\n",
    " \n",
    "\n",
    "Guidelines for coding console questions\n",
    "The lectures are interspersed with coding consoles to help you practise writing Python code. You will be given a brief problem statement and some pre-written code. You can write the code in the provided space, verify your answer using test cases, and submit when you are confident about the answer.\n",
    "\n",
    " \n",
    "\n",
    "Note that the coding console questions are non-graded. Some instructions for these questions are as follows:\n",
    "\n",
    "Ignore the pre-written code on the console. Please don't change it.\n",
    "Write your answer where you're asked to write it.\n",
    "You may run and verify your codes any number of times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NumPy Basics\n",
    "NumPy is a library written for scientific computing and data analysis. It stands for numerical python.\n",
    "\n",
    " \n",
    "\n",
    "The most basic object in NumPy is the ndarray, or simply an array which is an n-dimensional, homogeneous array. By homogenous, we mean that all the elements in a NumPy array have to be of the same data type, which is commonly numeric (float or integer).\n",
    "\n",
    " \n",
    "\n",
    "You can download the Python notebooks used in the lecture from the link below. It is recommended that you keep executing the commands on your computer at pace with the lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NumPy Arrays\n",
    "There are multiple ways to create NumPy arrays, the most common ones being:\n",
    "\n",
    "Convert lists or tuples to arrays using np.array(), as done above\n",
    "Initialise arrays of fixed size (when the size is known)\n",
    "The other common way is to initialise arrays. You do this when you know the size of the array beforehand.\n",
    "\n",
    "The following ways are commonly used:\n",
    "\n",
    "- np.ones(): Create array of 1s\n",
    "- np.zeros(): Create array of 0s\n",
    "- np.random.random(): Create array of random numbers\n",
    "- np.arange(): Create array with increments of a fixed step size\n",
    "- np.linspace(): Create array of fixed length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure and Content of Arrays\n",
    "It is helpful to inspect the structure of NumPy arrays, especially while working with large arrays. Some attributes of NumPy arrays are:\n",
    "\n",
    "- shape: Shape of array (n x m)\n",
    "- dtype: data type (int, float etc.)\n",
    "- ndim: Number of dimensions (or axes)\n",
    "- itemsize: Memory used by each array element in bytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset, Slice, Index and Iterate through Arrays\n",
    "Let's now look at how to access the elements of an array. For one-dimensional arrays, indexing, slicing etc. is similar to python lists - indexing starts at 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Arrays\n",
    "Multidimensional arrays are indexed using as many indices as the number of dimensions or axes. For instance, to index a 2-D array, you need two indices - array[x, y].\n",
    "\n",
    " \n",
    "\n",
    "Each axes has an index starting at 0. The following figure shows the axes and their indices for a 2-D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Times in NumPy and Standard Python Lists\n",
    "We mentioned that the key advantages of NumPy are convenience and speed of computation.\n",
    "\n",
    " \n",
    "\n",
    "You'll often work with extremely large datasets, and thus it is important to point for you to understand how much computation time (and memory) you can save using NumPy, compared to standard python lists.\n",
    "\n",
    " \n",
    "\n",
    "Let's compare the computation times of arrays and lists for a simple task of calculating the element-wise product of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this case, NumPy is an order of magnitude faster than lists. This is with arrays of size in millions, but you may work on much larger arrays of sizes in order of billions. Then, the difference is even larger.\n",
    "\n",
    " \n",
    "\n",
    "Some reasons for such difference in speed are:\n",
    "\n",
    "NumPy is written in C, which is basically being executed behind the scenes\n",
    "NumPy arrays are more compact than lists, i.e. they take much lesser storage space than lists\n",
    "The following discussions demonstrate the differences in speeds of NumPy and standard python:\n",
    "\n",
    "Why are NumPy arrays so fast?  https://stackoverflow.com/questions/8385602/why-are-numpy-arrays-so-fast\n",
    "Why NumPy instead of Python lists? https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Summary\n",
    "To conclude, in this session, you learnt about the most important package for scientific computing in Python NumPy. The various operations that you learnt about are â€”\n",
    "\n",
    "Arrays which are the basic data structure in NumPy library.\n",
    "Creating NumPy arrays from a list or a tuple.\n",
    "Creating randomly large arrays which can be done using the arange command.\n",
    "Analysing the shape and dimension of an array using array.shape, array.ndim and so on.\n",
    "Indexing, slicing and subsetting an array which is very similar to indexing in lists.\n",
    "Working on multidimensional arrays.\n",
    "Lastly, you studied the computation times in NumPy and standard Python lists and you concluded that NumPy arrays are faster than standard lists.\n",
    "\n",
    " \n",
    "\n",
    "In the next session, you will dive deep into various operations on NumPy arrays, and you will use these data structures to perform various tasks.\n",
    "\n",
    "Additional Reading http://www.numpy.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Arrays\n",
    "\n",
    "\n",
    "Introduction\n",
    "In this session we will go through the following:\n",
    "\n",
    "- Manipulate arrays\n",
    "- Reshape arrays\n",
    "- Stack arrays\n",
    "- Perform operations on arrays\n",
    "- Perform basic mathematical operations\n",
    "- Apply built-in functions\n",
    "- Apply your own functions\n",
    "- Apply basic linear algebra operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Operations\n",
    "Manipulating Arrays\n",
    "We can manipulate arrays, i.e. changing the shape, combining and splitting arrays, etc.\n",
    "\n",
    "Reshaping Arrays\n",
    "Reshaping is done using the reshape() function.\n",
    "\n",
    "Stacking and Splitting Arrays\n",
    "Stacking: np.hstack() and n.vstack()\n",
    "\n",
    "Stacking is done using the np.hstack() and np.vstack() methods. For horizontal stacking, the number of rows should be the same, while for vertical stacking, the number of columns should be the same.\n",
    "\n",
    " \n",
    "\n",
    "Note: The Python notebook used in this section can be downloaded from Introduction section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations on Arrays\n",
    "Performing mathematical operations on arrays is extremely simple. Let's see some common operations.\n",
    "\n",
    " \n",
    "\n",
    "Basic Mathematical Operations\n",
    "Numpy provides almost all the basic math functions - exp, sin, cos, log, sqrt etc. The function is applied to each element of the array.\n",
    "\n",
    " \n",
    "\n",
    "Applying User Defined Functions\n",
    "You can also apply your own functions on arrays. For e.g. applying the function x/(x+1) to each element of an array.\n",
    "\n",
    " \n",
    "\n",
    "One way to do that is by looping through the array, which is the non-numpy way. You would rather want to write vectorized code.\n",
    "\n",
    " \n",
    "\n",
    "The simplest way to do that is to vectorize the function you want, and then apply it on the array\n",
    "\n",
    "Numpy provides the np.vectorize() method to vectorize functions.\n",
    "\n",
    " \n",
    "\n",
    "Let's look at both the ways to do it.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Linear Algebra Operations\n",
    "NumPy provides the np.linalg package to apply common linear algebra operations, such as:\n",
    "\n",
    "- np.linalg.inv: Inverse of a matrix\n",
    "- np.linalg.det: Determinant of a matrix\n",
    "- np.linalg.eig: Eigenvalues and eigenvectors of a matrix\n",
    "- Also, you can multiply matrices using np.dot(a, b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "To conclude, in this session, you learnt about the various operation you may do on a NumPy array. The various operations that you learnt about are â€”\n",
    "\n",
    "Manipulating arrays which can be done using reshape().\n",
    "Stacking and splitting arrays which are similar to merging and appending and can be done using hstack(), vstack().\n",
    "Applying user-defined functions on an array to vectorized the code using np.vectorize().\n",
    "Performing linear algebra operations on an array like Inverse, Determinant or eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pandas\n",
    "Pandas is a library built using NumPy specifically for data analysis. You'll be using Pandas heavily for data manipulation, visualisation, building machine learning models, etc.\n",
    "\n",
    " \n",
    "\n",
    "There are two main data structures in Pandas - Series and Dataframes. The default way to store data is dataframes, and thus manipulating dataframes quickly is probably the most important skill set for data analysis.\n",
    "\n",
    "In this section, you will study:\n",
    "\n",
    "- The pandas Series (similar to a NumPy array)\n",
    "- Creating a pandas series\n",
    "- Indexing series\n",
    "- Dataframes\n",
    "- Creating dataframes from dictionaries\n",
    "- Importing CSV data files as pandas dataframes\n",
    "- Reading and summarising dataframes\n",
    "- Sorting dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Basics\n",
    "The Pandas Series\n",
    "A series is similar to a 1-D numpy array, and contains scalar values of the same type (numeric, character, datetime etc.). A dataframe is simply a table where each column is a pandas series.\n",
    "\n",
    "Creating Pandas Series\n",
    "Series are one-dimensional array-like structures, though unlike NumPy arrays, they often contain non-numeric data (characters, dates, time, booleans etc.)\n",
    "\n",
    " \n",
    "\n",
    "You can create pandas series from array-like objects using pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, you will work with Series only as a part of dataframes. Let's study the basics of dataframes.\n",
    "\n",
    "The Pandas Dataframe\n",
    "Dataframe is the most widely used data-structure in data analysis. It is a table with rows and columns, with rows having an index and columns having meaningful names.\n",
    "\n",
    "Creating dataframes from dictionaries\n",
    "There are various ways of creating dataframes, such as creating them from dictionaries, JSON objects, reading from txt, CSV files, etc.\n",
    "\n",
    " \n",
    "\n",
    "Note: The Python notebook used in this section can be downloaded from Introduction section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important concept in pandas dataframes is that of row indices. By default, each row is assigned indices starting from 0, and are represented at the left side of the dataframe.\n",
    "\n",
    " \n",
    "\n",
    "Let's now learn how we can change or manipulate the default indices and replace it with more sensible indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Selecting Data\n",
    "In this section, we will learn:\n",
    "\n",
    "- Select rows from a dataframe\n",
    "- Select columns from a dataframe\n",
    "- Select subsets of dataframes\n",
    "- Selecting Rows\n",
    "- Selecting rows in dataframes is similar to the indexing you have seen in NumPy arrays. The syntax df[start_index:end_index]will subset rows according to the start and end indices.\n",
    "\n",
    "Selecting Subsets of Dataframes\n",
    "Until now, you have seen selecting rows and columns using the following ways:\n",
    "\n",
    "- Selecting rows: df[start:stop]\n",
    "- Selecting columns: df['column'] or df.column or df[['col_x', 'col_y']]\n",
    "- df['column'] or df.column return a series\n",
    "- df[['col_x', 'col_y']] returns a dataframe\n",
    "\n",
    "\n",
    "But pandas does not prefer this way of indexing dataframes, since it has some ambiguity. For instance, let's try and select the third row of the dataframe.\n",
    "\n",
    "  \n",
    "  -------\n",
    "  \n",
    "  \n",
    "###  You have seen some ways of selecting rows and columns from dataframes. Let's now see some other ways of indexing dataframes, which pandas recommends, since they are more explicit (and less ambiguous).\n",
    "\n",
    "There are two main ways of indexing dataframes:\n",
    "\n",
    "Position based indexing using df.iloc\n",
    "Label based indexing using df.loc\n",
    "Using both the methods, we will do the following indexing operations on a dataframe:\n",
    "\n",
    "- Selecting single elements/cells\n",
    "- Selecting single and multiple rows\n",
    "- Selecting single and multiple columns\n",
    "- Selecting multiple rows and colum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### To summarise, df.iloc[x, y] uses integer indices starting at 0.\n",
    "\n",
    "The other common way of indexing is the label based indexing, which uses df.loc[].\n",
    "\n",
    "Label Based Indexing\n",
    "Pandas provides the df.loc[] functionality to index dataframes using labels.\n",
    "\n",
    "As mentioned in the documentation, the inputs x, y to df.loc[x, y] can be:\n",
    "\n",
    "- A single label, e.g. '3' or 'row_index'\n",
    "- A list or array of labels, e.g. ['3', '7', '8']\n",
    "- A range of labels, where row_x and row_y both are included, i.e. 'row_x':'row_y'\n",
    "- A boolean array Let's see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Append\n",
    "In this section, you will merge and concatenate multiple dataframes. Merging is one of the most common operations you will do, since data often comes in various files.\n",
    "\n",
    "In our case, we have sales data of a retail store spread across multiple files. We will now work with all these data files and learn to:\n",
    "\n",
    "Merge multiple dataframes using common columns/keys using pd.merge()\n",
    "Concatenate dataframes using pd.concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  You have seen how to do indexing of dataframes using df.iloc and df.loc. Now, let's see how to subset dataframes based on certain conditions.\n",
    "\n",
    "Subsetting Rows Based on Conditions\n",
    "Often, you want to select rows which satisfy some given conditions. For e.g., select all the orders where the Sales > 3000, or all the orders where 2000 < Sales < 3000 and Profit < 100.\n",
    "\n",
    "Arguably, the best way to do these operations is using df.loc[], since df.iloc[] would require you to remember the integer column indices, which is tedious.\n",
    "\n",
    "Let's see some more in the next lecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Summarizing Dataframes\n",
    "Grouping and aggregation are some of the most frequently used operations in data analysis, especially while doing exploratory data analysis (EDA), where comparing summary statistics across groups of data is common.\n",
    "\n",
    "For e.g., in the retail sales data we are working with, you may want to compare the average sales of various regions, or compare the total profit of two customer segments.\n",
    "\n",
    " \n",
    "\n",
    "Grouping analysis can be thought of as having three parts:\n",
    "\n",
    "Splitting the data into groups (e.g. groups of customer segments, product categories, etc.)\n",
    "Applying a function to each group (e.g. mean or total sales of each customer segment)\n",
    "Combining the results into a data structure showing the summary statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lecture we learned how to create a groupby object, let's now see how to use the created groupby object to carry out various aggregation in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda function & Pivot tables\n",
    "Until now, we have not made any changes or modifications to the data. In this section, we will:\n",
    "\n",
    "- Use lambda functions to create new and alter existing columns\n",
    "- Use pandas pivot tables as an alternative to df.groupby() to summarise data\n",
    "\n",
    "### Lambda Functions\n",
    "Say you want to create a new column indicating whether a given order was profitable or not (1/0).\n",
    "\n",
    "You need to apply a function which returns 1 if Profit > 0, else 0. This can be easily done using the apply() method on a column of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lecture, we will learn how we can create a new column using the existing columns in our dataframe. Columns which are created by the user are known as 'Derived Variables'. Derived variables increase the information conveyed by the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "You may want to use pandas pivot tables as an alternative to groupby(). They provide Excel-like functionalities to create aggregate tables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "To conclude, in this session, you learnt about the Pandas library which provides the various function to conduct data analysis in Python. The various topics that you learnt about are â€”\n",
    "\n",
    " \n",
    "\n",
    "Pandas series and dataframes which are the basic data structures in Pandas library.\n",
    "Indexing, Selecting and Subsetting a dataframe.\n",
    "Merging and appending two dataframes which can be done using .merge and .concat commands.\n",
    "Grouping and Summarising dataframes which can be done using groupby() to make an object then using this object to play around.\n",
    "Using Pivot table function on a dataframe which are similar to pivot tables provided in M.S Excel.\n",
    "In the next session, you will learn how to get data from various sources and cleaning it to use it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and Cleaning data\n",
    "### Introduction\n",
    "There are multiple ways of getting data into python, depending on where the data is stored. The simplest case is when you have data in CSV files, but often, you need to get data from other formats, sources and documents, such as text files, relational databases, websites, APIs, PDF documents, etc.\n",
    "\n",
    " \n",
    "\n",
    "In the following sections, you will learn to get data into python from a number of sources. You will learn to:\n",
    "\n",
    "- Get data from text files\n",
    "- Get data from relational databases\n",
    "- Scrape data from websites\n",
    "- Get data from publicly available APIs\n",
    "- Read PDFs into python\n",
    "\n",
    "In the process, you will also learn how to deal with nuances that inevitably come while getting data from various sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Delimited and Relational Databases\n",
    "- Getting Data From Delimited Files\n",
    "Delimited files are usually text files, where columns are separated by delimiters (such as commas, tabs, semicolons etc.) and each new line is a row.\n",
    "\n",
    " \n",
    "\n",
    "# Getting Data From Relational Databases\n",
    "Data is commonly stored in RDBMS, and it is easy to get it into Python. We'll use the most common one - MySQL.\n",
    "\n",
    "There are many libraries to connect MySQL and Python, such as pymysql, MySQLdb, etc. All of them follow the following procedure to connect to MySQL:\n",
    "\n",
    "- Create a connection object between MySQL and python\n",
    "- Create a cursor object (you use the cursor to open and close the connection)\n",
    "- Execute the SQL query\n",
    "- Retrieve results of the query using methods such as fetchone(),  fetchall(), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "    \n",
    "    http://jrogel.com/python-3-pandas-encoding-issues/\n",
    "   \n",
    "\n",
    "How will you find out the encoding scheme of your dataset if unknown?\n",
    "\n",
    "\n",
    "Will ask the developer\n",
    "\n",
    "Use the library chardet\n",
    "Feedback : Yes, we can use library chardet to identify the encoding scheme used in the given csv file.\n",
    "Correct\n",
    "\n",
    "Use read_csv to identify the encoding scheme automatic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data From Websites\n",
    "Web scraping refers to the art of programmatically getting data from the internet. One of the coolest features of python is that it makes it easy to scrape websites.\n",
    "\n",
    " \n",
    "\n",
    "In Python 3, the most popular library for web scraping is BeautifulSoup. To use BeautifulSoup, we will also need the requestsmodule, which basically connects to a given URL and fetches data from it (in HTML format). A web page is basically HTML code, and the main use of BeautifulSoup is that it helps you parse HTML easily.\n",
    "\n",
    "Note: Discussion on HTML syntax is beyond the scope of this module, though even very basic HTML experience should be enough to understand web scraping.\n",
    "\n",
    " Use Case - Fetching Mobile App Reviews from Google Playstore\n",
    "Let's say you want to understand why people install and uninstall mobile apps, and why they like or dislike certain apps. A very rich source of app-reviews data is the Google Playstore, where people write their feedback about the app.\n",
    "\n",
    " \n",
    "\n",
    "The reviews of the Facebook Messenger app can be found here: https://play.google.com/store/apps/details?id=com.facebook.orca&hl=en\n",
    "\n",
    " \n",
    "\n",
    "We will scrape reviews of the Messenger app, i.e. get them into python, and then you can do some interesting analyses on that.\n",
    "\n",
    "\n",
    "\n",
    "What are the two libraries you would need to scrape website data on Python?\n",
    "\n",
    "\n",
    "beautifulSoup\n",
    "requests\n",
    "Words2Note: Once submitted, answer is not editable.\n",
    "lightbulb_outline\n",
    "Suggested Answer\n",
    "\n",
    "1. Requests to connect to a URL and get data from it. 2. Then use BeautifulSoup to create an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Data From APIs\n",
    "APIs, or application programming interfaces, are created by companies and organisations to provide restricted access to data. It is very common to get data from APIs for data analysis, for example, you can get financial data (stock prices etc.), social media data (Facebook, Twitter etc. provide APIs), weather data, data about healthcare, music, food and drinks, and from almost every domain.\n",
    "\n",
    " \n",
    "\n",
    "Apart from being rich sources of data, there are other reasons to use APIs:\n",
    "\n",
    "When the data is being updated in real time. If you use downloaded CSV files, you'll have to download data manually and update your analysis multiple times. Through APIs, you can automate the process of getting real-time data.\n",
    "Easy access to structured and verified data - though you can scrape websites, APIs can directly provide data in structured format and is of better quality\n",
    "Access to restricted data: You cannot scrape all websites easily, and that's often illegal (e.g. Facebook, financial data etc.). APIs are the only way to get this data.\n",
    "There are many more reasons depending on the use cases and the domain of application.\n",
    "\n",
    "A list of useful APIs is available here: https://github.com/toddmotto/public-apis\n",
    "\n",
    "\n",
    "\n",
    "Example Use Case: Google Maps Geocoding API\n",
    "Google Maps provides many APIs, one of which is the Google Maps Geocoding API. You can use it to geocode addresses, i.e. get the latitude-longitude coordinates, and vice-versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs\n",
    "How will you read data from an API? Briefly, mention the steps involved.\n",
    "\n",
    "\n",
    "make api calls\n",
    "Words3Note: Once submitted, answer is not editable.\n",
    "lightbulb_outline\n",
    "Suggested Answer\n",
    "\n",
    "1. Join the words in the address by a plus and convert it to a form words+in+the+address 2. Connect to the URL by appending the address and the API key 3. Get a response from the API and convert it to a python object (here, a dictionary)\n",
    "\n",
    "#  Reading Data From PDF Files\n",
    "Reading PDF files is not as straightforward as reading text or delimited files, since PDFs often contain images, tables, etc. PDFs are mainly designed to be human-readable, and thus you need special libraries to read them in python (or any other programming language).\n",
    "\n",
    " \n",
    "\n",
    "Luckily, there are some really good libraries in Python. We will use PyPDF2 to read PDFs in Python since it is easy to use and works with most types of PDFs.\n",
    "\n",
    " \n",
    "\n",
    "Note that Python will only be able to read text from PDFs, not images, tables etc. (though that is possible using other specialised libraries).\n",
    "\n",
    " \n",
    "\n",
    "You can install PyPDF2 using pip install PyPDF2.\n",
    "\n",
    " \n",
    "\n",
    "For this illustration, we will read a PDF of the book 'Animal Farm' written by George Orwell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading PDF Files\n",
    "Read the document and answer the question.\n",
    "\n",
    "After creating a pdf file object, a pdf reader object, a page object which command you will use for extracting text from a pdf page?\n",
    "\n",
    "\n",
    "readText()\n",
    "\n",
    "extractText()\n",
    "Feedback : Yes, we can read a pdf page by using extractText() command.\n",
    "Correct\n",
    "\n",
    "getText()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Datasets\n",
    "In this section, we will study ways to identify and treat missing data. We will:\n",
    "\n",
    "- Identify missing data in dataframes\n",
    "- Treat (delete or impute) missing values\n",
    "- There are various reasons for missing data, such as, human-errors during data-entry, non availability at the end of the user (e.g. DOB of certain people), etc. Most often, the reasons are simply unknown.\n",
    "\n",
    " \n",
    "\n",
    "In python, missing data is represented using either of the two objects NaN (Not a Number) or NULL. We'll not get into the differences between them and how Python stores them internally etc. We'll focus on studying ways to identify and treat missing values in Pandas dataframes.\n",
    "\n",
    " \n",
    "\n",
    "There are four main methods to identify and treat missing data:\n",
    "\n",
    "- isnull(): Indicates presence of missing values, returns a boolean\n",
    "- notnull(): Opposite of isnull(), returns a boolean\n",
    "- dropna(): Drops the missing values from a data frame and returns the rest\n",
    "- fillna(): Fills (or imputes) the missing values by a specified value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the Melbourne house pricing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Missing Values\n",
    "The methods isnull() and notnull() are the most common ways of identifying missing values.\n",
    "\n",
    "- While handling missing data, you first need to identify the rows and columns containing missing values, count the number of missing values, and then decide how you want to treat them.\n",
    "\n",
    " \n",
    "\n",
    "- It is important that you treat missing values in each column separately, rather than implementing a single solution (e.g. replacing NaNs by the mean of a column) for all columns.\n",
    "\n",
    " \n",
    "\n",
    "- isnull() returns a boolean (True/False) which can then be used to find the rows or columns containing missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treating Missing Values\n",
    "There are broadly two ways to treat missing values:\n",
    "\n",
    "- Delete: Delete the missing values\n",
    "- Impute:\n",
    "Imputing by a simple statistic: Replace the missing values by another value, commonly the mean, median, mode etc.\n",
    "Predictive techniques: Use statistical models such as k-NN, SVM etc. to predict and impute missing values\n",
    "In general, imputation makes assumptions about the missing values and replaces missing values by arbitrary numbers such as mean, median etc. It should be used only when you are reasonably confident about the assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, deletion is often safer and recommended. You may lose some data, but will not make any unreasonable assumptions.\n",
    "\n",
    " \n",
    "\n",
    "Caution: Always have a backup of the original data if you're deleting missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the previous lecture we learned to handle missing values by deleting them, now let's learn how to impute them with the mean value of the overall column.\n",
    "\n",
    "Additional Reading\n",
    "How you treat missing values should ideally depend upon an understanding of why missing values occur. The reasons are classified into categories such as missing completely at random, missing at random, missingness that depends on the missing value itself etc.\n",
    "\n",
    " \n",
    "\n",
    "We'll not discuss why missing values occur, though you can read this article if interested.\n",
    "\n",
    "http://www.stat.columbia.edu/~gelman/arm/missing.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
